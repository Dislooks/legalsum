# MODELS CONFIGURATION
# Bei Entwicklung, Testing, Debugging und Erweiterung wurde Claude Sonnet 4.5 genutzt

# Azure OpenAI Konfiguration
azure_openai:
  endpoint_url: "${AZURE_OPENAI_ENDPOINT}"
  api_version: "${AZURE_OPENAI_API_VERSION}"
  api_key_env: "${AZURE_OPENAI_API_KEY}"
  
  models:
    gpt-4o-mini:
      deployment_name: "gpt-4o-mini"
      max_completion_tokens: 4096
      supports_streaming: true
      context_window: 128000
      api_type: "azure_openai"

    gpt-oss-120b:
      deployment_name: "gpt-oss-120b"
      max_completion_tokens: 32768
      supports_streaming: true
      context_window: 128000
      api_type: "azure_openai"

azure_ai_foundry:
  endpoint_url: "${AZURE_AI_ENDPOINT}"
  api_key_env: "${AZURE_AI_KEY}"
  
  models:
    Llama-4-Scout-17B-16E-Instruct:
      deployment_name: "Llama-4-Scout-17B-16E-Instruct"
      max_tokens: 32768
      supports_streaming: true
      context_window: 128000
      api_type: "ai_foundry"
      
    mistral-small-2503:
      deployment_name: "mistral-small-2503"
      max_tokens: 32768
      supports_streaming: true
      context_window: 128000
      api_type: "ai_foundry" 
      
    mistral-medium-2505:
      deployment_name: "mistral-medium-2505"
      max_tokens: 32768
      supports_streaming: true
      context_window: 128000
      api_type: "ai_foundry"
    
    Phi-4:
      deployment_name: "Phi-4"
      max_tokens: 4096
      supports_streaming: truea
      context_window: 16000
      api_type: "ai_foundry"                    

# Hugging Face Inference API Konfiguration
huggingface:
  models:
    Qwen/Qwen2.5-7B-Instruct:
      model_id: "Qwen/Qwen2.5-7B-Instruct"
      use_inference_endpoint: true
      inference_endpoint_url: "{HF_INFERENCE_ENDPOINT_URL}"
      max_new_tokens: 2048
      context_window: 4096
      requires_pro: false

    Qwen/Qwen2.5-32B-Instruct:
      model_id: "Qwen/Qwen3-14B"
      use_inference_endpoint: true
      inference_endpoint_url: "{HF_INFERENCE_ENDPOINT_URL}"
      max_new_tokens: 4096
      context_window: 32768
      requires_pro: false

# Standard-Experimentparameter
default_parameters:
  temperature: 0.7
  top_k: 50
  top_p: 0.9
  max_output_length: 2000

# Timeout- und Retry-Konfiguration
api_settings:
  timeout_seconds: 120
  max_retries: 3
  retry_delay_seconds: 5
  rate_limit_delay: 1.0  # Sekunden zwischen Anfragen
  batch_size: 5  # Anzahl paralleler Anfragen

# Logging-Level für verschiedene Komponenten
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  log_api_requests: true
  log_responses: false  # Vorsicht: kann sehr groß werden
  save_raw_outputs: true  # Rohe API-Antworten speichern